# CIDR ranges

# VPC CIDR block
export VPC_CIDR='10.160.128.0/18'

# edit the subnets blocks for each AZ:
# SUBNET_NAME=”AZ1/Metric,AZ2/Metric,AZ2/Metric”
export DMZ_SUBNETS="10.160.128.0/26,10.160.128.64/26,10.160.128.128/26"
export SERVICES_SUBNETS="10.160.136.0/21,10.160.144.0/21,10.160.152.0/23"  
export SERVICES_SUBNETS_NAME="Applications"  
# Added as a place holder until we can turn it off
export SECURITY_SUBNETS="10.160.191.0/28,10.160.191.16/28,10.160.191.32/28"  
export DATA_SUBNETS="10.160.160.0/23,10.160.162.0/23,10.160.164.0/23"
export WORKSTATIONS_SUBNETS="10.160.166.0/24,10.160.167.0/24,10.160.168.0/24"
export TESTBUILDSINFRA_SUBNETS=""
export TESTBUILDSAPP_SUBNETS=""
export TESTBUILDSDATA_SUBNETS=""
export REGION="eu-west-1"
export TF_VAR_azs="[\"${REGION}a\",\"${REGION}b\",\"${REGION}c\"]"

# Services account where the peered shared services live
export SERVICES_ACCOUNT="VMDB-PCS-NPD"

# Backend profile - The profile to be used as backend, i.e. for ANLTC use NPD/SBX
export BACKEND_PROFILE=${SERVICES_ACCOUNT}

# S3 bucket for initial backend (before consul deployed) 
# this will be used to create the bucket if it is missing.
export BACKEND_BUCKET="vmdb-anltc-poc-config"

# Defines which consul server to use, consul servers only deployed to PCS environments. I.e. For ANLTC vpc use NPD/SBX consul.
export CONSUL_PROFILE=${SERVICES_ACCOUNT}

# The elastic IP for the openvpn server
export OPENVPN_EIP="52.209.153.143"

# OpenVPN configuration
export OPENVPN_CONFIG_FILE="VMDB-PCS-NPD.ovpn"
export TF_VAR_openvpn_user='mvc'
export TF_VAR_openvpn_password='HBU767f7B6xcsywd72@'

# SSH cidr range IP list of net blocks allowed to ssh to instances optional and should be limited and superceded by security groups as the build progresses
export TF_VAR_ssh_cidrs="[\"193.32.48.14/32\"]" #Example VPC cidr and VM VPN IP
export TF_VAR_static_routes='193.32.48.14/32'


# components - control which component is deployed, set to TRUE or FALSE
export DEPLOY_IAM_ROLES=TRUE
export DEPLOY_VPC=TRUE
export DEPLOY_VPC_PEERING=TRUE
export DEPLOY_SQUID=TRUE
export DEPLOY_CONSUL=FALSE
export DEPLOY_VAULT=FALSE
export DEPLOY_CW_METRICS=TRUE
export DEPLOY_DEVOPS=FALSE

# Peering 
export VPCPEER_ACCEPTER_ACCT_ID="160037550773" # the account number for the remote VPC (i.e. NPD)
export VPCPEER_ACCEPTER_VPC_CIDR="10.160.64.0/19" # the CIDR block of the remote VPC (i.e. NPD vpc)
export VPCPEER_ACCEPTER_VPC_ID="vpc-15119872" # the ID remote VPC

# PCS peering ID can be found in the ouputs from VPC peering module
export TF_VAR_pcs_vpc_peering_id="pcx-3682e35f" # PCS peering ID can be found in the ouputs from VPC peering module

export TF_VAR_pcs_vpc_cidr=$VPCPEER_ACCEPTER_VPC_CIDR #TECHDEBT

export VPCPEER_AUTOROUTE="true"

#TECHDEBT - currently the vpc peering will set a route between the entire VPCs CIDR blocks. probably better to limit it by subnets.

export ENV_AMI_ID="ami-2bbb4952"

# VMDB VPN endpoints
# export TF_VAR_vpn_conn_config="[\"193.32.46.1|65000\"]" manually built in ANLTC
# Manually created VPG added to route for propagation [OPTIONAL]
export TF_VAR_external_gw_id='vgw-986558ec'


# Share App settings
export TF_VAR_workstation_sg="sg-876f2aff"

#  Rstudio
export TF_VAR_rstudio_instance_type=m4.2xlarge
export TF_VAR_rstudio_ami_id="ami-ad5e9bd4"
export TF_VAR_iam_instance_profile=VMDB-RStudio-service-role

# Talend
export TF_VAR_talend_instance_type=m4.2xlarge
export TF_VAR_talend_ami_id="ami-7c5a9f05"
export TF_VAR_talend_iam_instance_profile="VMDB-Talend-service-role"

# Rstudio RDS
export TF_VAR_skip_final_snapshot="false"
export TF_VAR_multi_az="true"
export TF_VAR_db_name=PGDB01
export TF_VAR_db_identifier=vmdbanltcpocpgdb01
export TF_VAR_db_instance_type=db.m4.2xlarge
export TF_VAR_rds_allocated_storage=1515
export TF_VAR_db_backups=10

export TF_VAR_kms_general_key="arn:aws:kms:eu-west-1:557746599547:key/6340e58b-ec24-4628-a362-db43aad9e1dc"
export TF_VAR_kms_ami_key="arn:aws:kms:eu-west-1:557746599547:key/15ddc58e-bc31-4654-9d83-7b1b263781ea"

